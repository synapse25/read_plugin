{
  "name": "@knod/unfluff",
  "version": "1.3.1",
  "description": "A web page content extractor based on https://github.com/ageitgey/node-unfluff, but ready for browserify",
  "homepage": "https://github.com/knod/node-unfluff",
  "keywords": [
    "content extraction",
    "html",
    "scraping",
    "scrape",
    "web page",
    "body text",
    "article"
  ],
  "author": {
    "name": "knod"
  },
  "repository": {
    "type": "git",
    "url": "git://github.com/knod/node-unfluff.git"
  },
  "bugs": {
    "url": "https://github.com/knod/node-unfluff/issues"
  },
  "engines": {
    "node": "0.8.x || 0.9.x || 0.10.x"
  },
  "main": "lib/unfluff.js",
  "directories": {
    "bin": "bin",
    "lib": "lib",
    "test": "test"
  },
  "dependencies": {
    "cheerio": "~0.17.0",
    "optimist": "~0.6.1",
    "lodash": "~2.4.1",
    "xregexp": "~2.0.0"
  },
  "devDependencies": {
    "coffee-script-redux": "2.0.0-beta7",
    "commonjs-everywhere": "0.9.x",
    "mocha": "~1.12.1",
    "scopedfs": "~0.1.0",
    "semver": "~2.1.0",
    "deep-equal": "~0.2.1"
  },
  "scripts": {
    "test": "make test"
  },
  "licenses": [
    {
      "type": "Apache",
      "url": "https://github.com/knod/node-unfluff/blob/master/LICENSE"
    }
  ],
  "bin": {
    "unfluff": "bin/unfluff"
  },
  "readme": "# unfluff\n\nAn automatic web page content extractor for Node.js!\n\n[![Build Status](https://travis-ci.org/knod/node-unfluff.svg?branch=master)](https://travis-ci.org/knod/node-unfluff)\n\nAutomatically grab the main text out of a webpage like this:\n\n```\nextractor = require('unfluff');\ndata = extractor(my_html_data);\nconsole.log(data.text);\n```\n\nIn other words, it turns pretty webpages into boring plain text/json data:\n\n![](https://cloud.githubusercontent.com/assets/896692/3478577/b82f39cc-033d-11e4-9e68-226c9a7bc1c0.jpg)\n\nThis might be useful for:\n- Writing your own Instapaper clone\n- Easily building ML data sets from web pages\n- Reading your favorite articles from the console?\n\nPlease don't use this for:\n- Stealing other peoples' web pages\n- Making crappy spam sites with stolen content from other sites\n- Being a jerk\n\n## Credits / Thanks\n\nThis module is a front-endable (with browserify) version of [unflufff by ageitgey](https://github.com/ageitgey/node-unfluff) who has this to say about the tool: This library is largely based on [python-goose](https://github.com/grangier/python-goose) by [Xavier Grangier](https://github.com/grangier) which is in turn based on [goose](https://github.com/GravityLabs/goose) by [Gravity Labs](https://github.com/GravityLabs). However, it's not an exact port so it may behave differently on some pages and the feature set is a little bit different.  If you are looking for a python or Scala/Java/JVM solution, check out those libraries!\n\nThis README is, in most part, the same as ageitgey's. This fork simply removes the use of the `fs` module, allowing the tool to be browserified and used in the front end as well as the backend. I hope the module will be replaced by an fs-free version of the original, but for now here it is.\n\n## Install\n\nTo install the command-line `unfluff` utility:\n\n    npm install -g @knod/unfluff\n\nTo install the `unfluff` module for use in your Node.js project:\n\n    npm install --save @knod/unfluff\n\n## Usage\n\nYou can use `unfluff` from node or right on the command line!\n\n### Extracted data elements\n\nThis is what `unfluff` will try to grab from a web page:\n- `title` - The document's title (from the &lt;title&gt; tag)\n- `softTitle` - A version of `title` with less truncation\n- `date` - The document's publication date\n- `copyright` - The document's copyright line, if present\n- `author` - The document's author\n- `publisher` - The document's publisher (website name)\n- `text` - The main text of the document with all the junk thrown away\n- `image` - The main image for the document (what's used by facebook, etc.)\n- `videos` - An array of videos that were embedded in the article. Each video has src, width and height.\n- `tags`- Any tags or keywords that could be found by checking &lt;rel&gt; tags or by looking at href urls.\n- `canonicalLink` - The [canonical url](https://support.google.com/webmasters/answer/139066?hl=en) of the document, if given.\n- `lang` - The language of the document, either detected or supplied by you.\n- `description` - The description of the document, from &lt;meta&gt; tags\n- `favicon` - The url of the document's [favicon](http://en.wikipedia.org/wiki/Favicon).\n- `links` - An array of links embedded within the article text. (text and href for each)\n\nThis is returned as a simple json object.\n\n### Command line interface\n\nYou can pass a webpage to unfluff and it will try to parse out the interesting\nbits.\n\nYou can either pass in a file name:\n\n```\nunfluff my_file.html\n```\n\nOr you can pipe it in:\n\n```\ncurl -s \"http://somesite.com/page\" | unfluff\n```\n\nYou can easily chain this together with other unix commands to do cool stuff.\nFor example, you can download a web page, parse it and then use\n[jq](http://stedolan.github.io/jq/) to print it just the body text.\n\n```\ncurl -s \"http://www.polygon.com/2014/6/26/5842180/shovel-knight-review-pc-3ds-wii-u\" | unfluff | jq -r .text\n```\n\nAnd here's how to find the top 10 most common words in an article:\n\n```\ncurl -s \"http://www.polygon.com/2014/6/26/5842180/shovel-knight-review-pc-3ds-wii-u\" | unfluff |  tr -c '[:alnum:]' '[\\n*]' | sort | uniq -c | sort -nr | head -10\n```\n\n### Module Interface\n\n#### `extractor(html, language)`\n\nhtml: The html you want to parse\n\nlanguage (optional): The document's two-letter language code. This will be\nauto-detected as best as possible, but there might be cases where you want to\noverride it.\n\nThe extraction algorithm depends heavily on the language, so it probably won't work\nif you have the language set incorrectly.\n\n```javascript\nextractor = require('unfluff');\n\ndata = extractor(my_html_data);\n```\n\nOr supply the language code yourself:\n\n```javascript\nextractor = require('unfluff');\n\ndata = extractor(my_html_data, 'en');\n```\n\n`data` will then be a json object that looks like this:\n\n```json\n{\n  \"title\": \"Shovel Knight review\",\n  \"softTitle\": \"Shovel Knight review: rewrite history\",\n  \"date\": \"2014-06-26T13:00:03Z\",\n  \"copyright\": \"2016 Vox Media Inc Designed in house\",\n  \"author\": [\n    \"Griffin McElroy\"\n  ],\n  \"publisher\": \"Polygon\",\n  \"text\": \"Shovel Knight is inspired by the past in all the right ways — but it's far from stuck in it. [.. snip ..]\",\n  \"image\": \"http://cdn2.vox-cdn.com/uploads/chorus_image/image/34834129/jellyfish_hero.0_cinema_1280.0.png\",  \n  \"tags\": [],\n  \"videos\": [],\n  \"canonicalLink\": \"http://www.polygon.com/2014/6/26/5842180/shovel-knight-review-pc-3ds-wii-u\",\n  \"lang\": \"en\",\n  \"description\": \"Shovel Knight is inspired by the past in all the right ways — but it's far from stuck in it.\",\n  \"favicon\": \"http://cdn1.vox-cdn.com/community_logos/42931/favicon.ico\",\n  \"links\": [\n    { \"text\": \"Six Thirty\", \"href\": \"http://www.sixthirty.co/\" }\n  ]\n}\n```\n\n#### `extractor.lazy(html, language)`\n\nLazy version of `extractor(html, language)`.\n\nThe text extraction algorithm can be somewhat slow on large documents.  If you\nonly need access to elements like `title` or `image`, you can use the\nlazy extractor to get them more quickly without running the full processing\npipeline.\n\nThis returns an object just like the regular extractor except all fields\nare replaced by functions and evaluation is only done when you call those\nfunctions.\n\n```javascript\nextractor = require('unfluff');\n\ndata = extractor.lazy(my_html_data, 'en');\n\n// Access whichever data elements you need directly.\nconsole.log(data.title());\nconsole.log(data.softTitle());\nconsole.log(data.date());\nconsole.log(data.copyright());\nconsole.log(data.author());\nconsole.log(data.publisher());\nconsole.log(data.text());\nconsole.log(data.image());\nconsole.log(data.tags());\nconsole.log(data.videos());\nconsole.log(data.canonicalLink());\nconsole.log(data.lang());\nconsole.log(data.description());\nconsole.log(data.favicon());\n```\n\nSome of these data elements require calculating intermediate representations\nof the html document. Everything is cached so looking up multiple data elements\nand looking them up multiple times should be as fast as possible.\n\n### Demo\n\nThe easiest way to try out `unfluff` is to just install it:\n\n```\n$ npm install -g unfluff\n$ curl -s \"http://www.cnn.com/2014/07/07/world/americas/mexico-earthquake/index.html\" | unfluff\n```\n\nBut if you can't be bothered, you can check out\n[fetch text](http://fetchtext.herokuapp.com/). It's a site by\n[Andy Jiang](https://twitter.com/andyjiang) that uses `unfluff`. You send an\nemail with a url and it emails back with the cleaned content of that url. It\nshould give you a good idea of how `unfluff` handles different urls.\n\n### What is broken\n\n- Parsing web pages in languages other than English is poorly tested and probably\n  is buggy right now.\n- This definitely won't work yet for languages like Chinese / Arabic / Korean /\n  etc that need smarter word tokenization.\n- This has only been tested on a limited set of web pages. There are probably lots\n  of lurking bugs with web pages that haven't been tested yet.\n",
  "readmeFilename": "README.md",
  "gitHead": "ac6b5021805b3e8aab6c401ca7397eaf97090324",
  "_id": "@knod/unfluff@1.3.1",
  "_shasum": "a5ad4a85ca44e16fe97530c8ce75563996ba45f0",
  "_from": "@knod/unfluff@*"
}
